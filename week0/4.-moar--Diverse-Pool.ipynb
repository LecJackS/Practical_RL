{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\* This notebook is a continuation of *[FrozenLake.ipynb](./frozenlake.ipynb)*\n",
    "## \\* The first part of the code is all the same\n",
    "## \\* Starting *@[moar](#moar)* you'll find whats changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# moar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the genetic algorithm aren't optimal, try to find something better. (size, crossovers and mutations)\n",
    "\n",
    "Try alternative crossover and mutation strategies\n",
    "\n",
    "* prioritize crossover for higher-scorers?\n",
    "\n",
    "# Â·  try to select a more diverse pool, not just best scorers?  (we'll do this one here)\n",
    "* Just tune the f*cking probabilities.\n",
    "\n",
    "See which combination works best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick explanation\n",
    "\n",
    "We'll do the same best scores to crossover as before, but we'll change two things while selecting best scorers:\n",
    "1. The policies need to be unique.\n",
    "2. With some probability, we'll select a random policy in the final pool but not into the best scorers. (this didn't worked so well. I just keep the code as experimentation.)\n",
    "3. mutations will not change the actual policy selected, so we'll have a policy x and its mutated policy y, and keep both of them\n",
    "\n",
    "Jump to [Diverse Pool, where we change the code to do that (click here)](#Diverse-Pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations\n",
    "**Similar code as before:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-12 20:42:37,853] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "#create a single game instance\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "#start new game\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll modify get_random_policy() to return not repeated policies in the pool.\n",
    "\n",
    "If no pool is passed to this function, it will return any random policy, as before.\n",
    "\n",
    "**NOTE**: We'll NOT use this in this notebook (maybe never). I find it very ugly to do this in this way. It requires to modify lot of code, so I'll just remove duplicates in the final pool. Maybe it's not the best, but it's easier to understand and the final product will be (probably) very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "def get_random_policy(pool=[]):\n",
    "    \"\"\"\n",
    "    Build a numpy array representing agent policy.\n",
    "    This array must have one element per each of 16 environment states.\n",
    "    Element must be an integer from 0 to 3, representing action\n",
    "    to take from that state.\n",
    "    \"\"\"\n",
    "    # randint(0, 4, 16) returns an array of integers between 0 and 3 inclusive\n",
    "    # (or, in other words, starting from 0 to below 4)\n",
    "    # and the third therm (16), will be the array size\n",
    "    rand_pol = np.random.randint(0, n_actions, n_states)\n",
    "    while rand_pol in pool:\n",
    "        #it will loop until a NEW random policy appear\n",
    "        print(\"rand_pol:\", rand_pol)\n",
    "        print(\"pool: \", pool)\n",
    "        rand_pol = np.random.randint(0, n_actions, n_states)\n",
    "        \n",
    "    return rand_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action frequencies over 10^4 samples: [ 0.25014375  0.25130625  0.2495375   0.2490125 ]\n",
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "policies = [get_random_policy() for i in range(10**4)]\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should match n_actions-1'\n",
    "action_probas = np.unique(policies, return_counts=True)[-1] /10**4. /n_states\n",
    "print(\"Action frequencies over 10^4 samples:\",action_probas)\n",
    "assert np.allclose(action_probas, [1. / n_actions] * n_actions, atol=0.05), \"The policies aren't uniformly random (maybe it's just an extremely bad luck)\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate!\n",
    "* Implement a simple function that runs one game and returns the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_reward(env, policy, t_max=100):\n",
    "    \"\"\"\n",
    "    Interact with an environment, return sum of all rewards.\n",
    "    If game doesn't end on t_max (e.g. agent walks into a wall), \n",
    "    force end the game and return whatever reward you got so far.\n",
    "    Tip: see signature of env.step(...) method above.\n",
    "    \"\"\"\n",
    "    #s: state; where our actor is\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # p = policy: with probabilities equal to the ones returned by get_random_policy()\n",
    "        s, reward, is_done, _ = env.step(policy[s])\n",
    "        # accumulating rewards\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 10^3 sessions...\n",
      "Looks good!\n"
     ]
    }
   ],
   "source": [
    "print(\"generating 10^3 sessions...\")\n",
    "rewards = [sample_reward(env,get_random_policy()) for _ in range(10**3)]\n",
    "assert all([type(r) in (int, float) for r in rewards]), 'sample_reward must return a single number'\n",
    "assert all([0 <= r <= 1 for r in rewards]), 'total rewards should be between 0 and 1 for frozenlake (if solving taxi, delete this line)'\n",
    "print(\"Looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(policy, n_times=100):\n",
    "    \"\"\"Run several evaluations and average the score the policy gets.\"\"\"\n",
    "    # rewards: array with n_times (100) elements consisting of the total_rewards returned by sample_reward()\n",
    "    rewards = [sample_reward(env, policy) for n in range(n_times)]\n",
    "    return float(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignoring the random search, jumping right into\n",
    "# Part II - Genetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random policy:\n",
      "v > > v\n",
      "^ H v H\n",
      "v < v H\n",
      "H v > G\n"
     ]
    }
   ],
   "source": [
    "def print_policy(policy):\n",
    "    \"\"\"a function that displays a policy in a human-readable way.\"\"\"\n",
    "    lake = \"SFFFFHFHFFFHHFFG\"\n",
    "    assert env.spec.id == \"FrozenLake-v0\", \"this function only works with frozenlake 4x4\"\n",
    "\n",
    "    \n",
    "    # where to move from each tile (we're a bit unsure if this is accurate)\n",
    "    arrows = ['>^v<'[a] for a in policy]\n",
    "    \n",
    "    #draw arrows above S and F only\n",
    "    signs = [arrow if tile in \"SF\" else tile for arrow, tile in zip(arrows, lake)]\n",
    "    \n",
    "    for i in range(0, 16, 4):\n",
    "        print(' '.join(signs[i:i+4]))\n",
    "\n",
    "print(\"random policy:\")\n",
    "print_policy(get_random_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossover(policy1, policy2, p=0.5):\n",
    "    \"\"\"\n",
    "    for each state, with probability p take action from policy1, else policy2\n",
    "    \"\"\"\n",
    "    # policyx: [0,1,3,2,1,0,3,2,1,0,3,2,0,2,2,1]\n",
    "    new_pol = []\n",
    "    for i in range(len(policy1)):\n",
    "        #choosing between the ith element between pol1 and pol2 with probability p\n",
    "        new_pol.append(np.random.choice((policy1[i], policy2[i]), p=[p, 1-p]))\n",
    "        \n",
    "    return new_pol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll modify this function to return a copy of the mutated policy and not replace it directly on the selected policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutation(policy, p=0.01):\n",
    "    \"\"\"\n",
    "    for each state, with probability p replace action with random action\n",
    "    Tip: mutation can be written as crossover with random policy\n",
    "    \"\"\"\n",
    "    # if we modify \"policy\" directly, we'll change the value of policy. Lists work that way, so we\n",
    "    # need to use a copy\n",
    "    mutated_policy = list(policy)\n",
    "    #n_actions = env.action_space.n\n",
    "    for a in policy:\n",
    "        # with 1% probability, we mutate element a from policy\n",
    "        if np.random.choice((0,1), p=[1-p, p]):\n",
    "            mutated_policy[a] = np.random.randint(0, n_actions)\n",
    "    return mutated_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "policies = [crossover(get_random_policy(), get_random_policy()) \n",
    "            for i in range(10**4)]\n",
    "\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should be n_actions-1'\n",
    "\n",
    "assert any([np.mean(crossover(np.zeros(n_states), np.ones(n_states))) not in (0, 1)\n",
    "               for _ in range(100)]), \"Make sure your crossover changes each action independently\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add n_low_scorers to control how many random low scorers will be added to the final pool.\n",
    "\n",
    "This was an experimentation. It result as pure shit, so now the default is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20 #default: 100 - how many cycles to make\n",
    "pool_size = 100 #how many policies to maintain\n",
    "n_crossovers = 50 #how many crossovers to make on each step\n",
    "n_mutations = 50 #how many mutations to make on each tick\n",
    "n_low_scorers = 0 #how many random low-scorer policies, not into the best scorers, can pass to the next pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing...\n"
     ]
    }
   ],
   "source": [
    "print(\"initializing...\")\n",
    "#pool = <spawn a list of pool_size random policies>\n",
    "pool = [get_random_policy() for i in range(pool_size)]\n",
    "#pool_scores = <evaluate every policy in the pool, return list of scores>\n",
    "pool_scores = [evaluate(p) for p in pool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(pool) == type(pool_scores) == list\n",
    "assert len(pool) == len(pool_scores) == pool_size\n",
    "assert all([type(score) in (float, int) for score in pool_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diverse Pool\n",
    "\n",
    "## Changing some code below here.\n",
    "### crossovered now prioritize higher scores as 3.-moar\n",
    "### now we'll:\n",
    "#### 1. Remove duplicates\n",
    "#### 2. With probability p, select a random policy from the final pool but not one into the best scorers.\n",
    "#### 3. mutations will not change the actual policy selected, so we'll have a policy x and its mutated policy y, and keep both of them\n",
    "#### 4. crossovers will be between a high scorer and a random policy from the pool, to prevent crossovers between very similar policies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Best score: 0.15\n",
      "> < < <\n",
      "v H v H\n",
      "v ^ ^ H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 1:\n",
      "We found 15 duplicated policies at this epoch!\n",
      "Best score: 0.15\n",
      "< < > v\n",
      "< H > H\n",
      "v v ^ H\n",
      "H < v G\n",
      "\n",
      "Epoch 2:\n",
      "We found 16 duplicated policies at this epoch!\n",
      "Best score: 0.21\n",
      "^ < > v\n",
      "> H > H\n",
      "v v > H\n",
      "H v < G\n",
      "\n",
      "Epoch 3:\n",
      "We found 23 duplicated policies at this epoch!\n",
      "Best score: 0.21\n",
      "< < > >\n",
      "^ H ^ H\n",
      "v v > H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 4:\n",
      "We found 15 duplicated policies at this epoch!\n",
      "Best score: 0.37\n",
      "^ < > >\n",
      "> H > H\n",
      "< ^ > H\n",
      "H ^ v G\n",
      "\n",
      "Epoch 5:\n",
      "We found 27 duplicated policies at this epoch!\n",
      "Best score: 0.33\n",
      "> v > <\n",
      "> H v H\n",
      "< ^ ^ H\n",
      "H ^ ^ G\n",
      "\n",
      "Epoch 6:\n",
      "We found 18 duplicated policies at this epoch!\n",
      "Best score: 0.42\n",
      "^ < > >\n",
      "> H > H\n",
      "< ^ > H\n",
      "H ^ v G\n",
      "\n",
      "Epoch 7:\n",
      "We found 16 duplicated policies at this epoch!\n",
      "Best score: 0.32\n",
      "^ < > >\n",
      "> H > H\n",
      "< ^ > H\n",
      "H ^ v G\n",
      "\n",
      "Epoch 8:\n",
      "We found 15 duplicated policies at this epoch!\n",
      "Best score: 0.64\n",
      "> < > <\n",
      "> H v H\n",
      "< ^ ^ H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 9:\n",
      "We found 22 duplicated policies at this epoch!\n",
      "Best score: 0.61\n",
      "> < > <\n",
      "> H v H\n",
      "< ^ ^ H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 10:\n",
      "We found 21 duplicated policies at this epoch!\n",
      "Best score: 0.75\n",
      "> < > <\n",
      "> H v H\n",
      "< ^ ^ H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 11:\n",
      "We found 18 duplicated policies at this epoch!\n",
      "Best score: 0.72\n",
      "> < > v\n",
      "> H > H\n",
      "< ^ > H\n",
      "H v v G\n",
      "\n",
      "Epoch 12:\n",
      "We found 18 duplicated policies at this epoch!\n",
      "Best score: 0.67\n",
      "> < > >\n",
      "> H > H\n",
      "< ^ > H\n",
      "H v v G\n",
      "\n",
      "Epoch 13:\n",
      "We found 19 duplicated policies at this epoch!\n",
      "Best score: 0.68\n",
      "> < > v\n",
      "> H > H\n",
      "< ^ > H\n",
      "H v v G\n",
      "\n",
      "Epoch 14:\n",
      "We found 27 duplicated policies at this epoch!\n",
      "Best score: 0.75\n",
      "> < < <\n",
      "> H v H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 15:\n",
      "We found 23 duplicated policies at this epoch!\n",
      "Best score: 0.77\n",
      "> < < <\n",
      "> H v H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 16:\n",
      "We found 24 duplicated policies at this epoch!\n",
      "Best score: 0.8\n",
      "> < > <\n",
      "> H v H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 17:\n",
      "We found 31 duplicated policies at this epoch!\n",
      "Best score: 0.84\n",
      "> < > >\n",
      "> H v H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 18:\n",
      "We found 21 duplicated policies at this epoch!\n",
      "Best score: 0.88\n",
      "> < > >\n",
      "> H > H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "\n",
      "Epoch 19:\n",
      "We found 20 duplicated policies at this epoch!\n",
      "Best score: 0.82\n",
      "> < > <\n",
      "> H > H\n",
      "< ^ > H\n",
      "H v ^ G\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#main loop\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch %s:\"%epoch)\n",
    "    \n",
    "    # 1. Removing duplicates from pool\n",
    "    # converting list of np arrays to list of tuples because I couldn't find a better way to remove duplicates\n",
    "    uniques_pool = [tuple(policy) for policy in pool]\n",
    "    # set() returns only unique elements. We need to convert them to np arrays again\n",
    "    uniques_pool = [np.asarray(policy) for policy in set(uniques_pool)]\n",
    "    if len(uniques_pool) != len(pool):\n",
    "        # We found some duplicates\n",
    "        print(\"We found\", len(pool) - len(uniques_pool), \"duplicated policies at this epoch!\")\n",
    "        pool = uniques_pool\n",
    "    # We could check for duplicates on crossovered or mutated,\n",
    "    #but it's more similar code and I want to finish this one\n",
    "    \n",
    "    # evaluation policies before crossovering them:\n",
    "    pool_scores = [evaluate(p) for p in pool]\n",
    "    # we'll select the best n_crossovers (50) policies to mix between them\n",
    "    # we could use another number of best policies instead of n_crossovers (50),\n",
    "    # but it's late and I don't know what I'm doing\n",
    "    selected_indices = np.argsort(pool_scores)[-n_crossovers:]\n",
    "    \n",
    "    #crossovered = <crossover random guys from pool, n_crossovers total>\n",
    "    # using selected_indices as a fraction of pool with 50 best scores\n",
    "    crossovered = [crossover(pool[np.random.choice(selected_indices)], \n",
    "                             pool[np.random.choice(len(pool))]) \n",
    "                   for c in range(n_crossovers)]\n",
    "    # from now on it's all the same: mutations, adding all to a pool, evaluating (again) and selecting\n",
    "    # best scores. Repeat for n_epochs.\n",
    "    #mutated = <add several new policies at random, n_mutations total>\n",
    "    mutated = [mutation(pool[np.random.choice(len(pool))]) \n",
    "               for m in range(n_mutations)]\n",
    "    \n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "    \n",
    "    #add new policies to the pool\n",
    "    #pool = <add up old population with crossovers/mutations>\n",
    "    #plus sing (+) concatenates lists in python\n",
    "    pool = pool + crossovered + mutated\n",
    "    #pool_scores = <evaluate all policies again>\n",
    "    pool_scores = [evaluate(p) for p in pool]\n",
    "    \n",
    "    # 2. Adding a couple of random low scorers to the final pool\n",
    "    # select pool_size-n_low_scorers best policies. we'll add n_low_scorers later \n",
    "    selected_indices = np.argsort(pool_scores)[-pool_size+n_low_scorers:]\n",
    "    # Now we need to add n_low_scorers to our indices\n",
    "    # np.argsort(pool_scores)[:-pool_size+n_low_scorers] will contain all indices NOT used abobe\n",
    "    # so we need to choose n_low_scorers random indices from there\n",
    "    low_scorers_indices = np.random.choice(np.argsort(pool_scores)[:-pool_size+n_low_scorers], n_low_scorers)\n",
    "    # now we need to concatenate all indices into one numpy array\n",
    "    selected_indices = np.concatenate((selected_indices, low_scorers_indices))\n",
    "    #filling pool only with best values\n",
    "    pool = [pool[i] for i in selected_indices]\n",
    "    pool_scores = [pool_scores[i] for i in selected_indices]\n",
    "\n",
    "    #print the best policy so far (last in ascending score order)\n",
    "    print(\"Best score:\", pool_scores[-1])\n",
    "    print_policy(pool[-1])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
