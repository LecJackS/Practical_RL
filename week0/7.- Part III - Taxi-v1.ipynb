{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III (4 points +)\n",
    "\n",
    "The frozenlake problem above is just too simple: you can beat it even with a random policy search. Go solve something more complicated.\n",
    "\n",
    "Pick __one of the two tasks__:\n",
    "\n",
    "* __FrozenLake8x8-v0__ - frozenlake big brother. Achieve score >0.7\n",
    "* __Taxi-v1__ - essentially a maze where you get score for moving passengers to their destinations. Achieve score >-100)\n",
    "\n",
    "Your homework assignment is beating that score (see tips below).\n",
    "\n",
    "\n",
    "### Some tips:\n",
    "* When solving those envs, please make sure your t_max is large enough to finish game with suboptimal policy. For example, __Taxi-v0 only trains if you let it play for 10k+ ticks/session__. For frozenlake8x8 it's less dire.\n",
    "* Random policy search is worth trying as a sanity check, but in general you should expect the genetic algorithm (or anything you devised in it's place) to fare much better that random.\n",
    "* While _it's okay to adapt the tabs above to your chosen env_, make sure you didn't hard-code any constants there (e.g. 16 states or 4 actions).\n",
    "* `print_policy` function was built for the frozenlake-v0 env so it will break on any other env. You could simply ignore it or rewrite it for your env.\n",
    "* in function `sample_reward`, __make sure t_max steps is enough to solve the environment__ even if agent is sometimes acting suboptimally. To estimate that, run several sessions without time limit and measure their length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations\n",
    "**Similar code as before:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-11 18:43:26,561] Making new env: Taxi-v2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "#from gym import wrappers\n",
    "\n",
    "#create a single game instance\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "#env = wrappers.Monitor(env, '/tmp/frozenlake8x8-v0')\n",
    "\n",
    "#start new game\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "def get_random_policy(pool=[]):\n",
    "    \"\"\"\n",
    "    Build a numpy array representing agent policy.\n",
    "    This array must have one element per each of 16 environment states.\n",
    "    Element must be an integer from 0 to 3, representing action\n",
    "    to take from that state.\n",
    "    \"\"\"\n",
    "    # randint(0, 4, 16) returns an array of integers between 0 and 3 inclusive\n",
    "    # (or, in other words, starting from 0 to below 4)\n",
    "    # and the third therm (16), will be the array size\n",
    "    rand_pol = np.random.randint(0, n_actions, n_states)\n",
    "    while rand_pol in pool:\n",
    "        #it will loop until a NEW random policy appear\n",
    "        print(\"rand_pol:\", rand_pol)\n",
    "        print(\"pool: \", pool)\n",
    "        rand_pol = np.random.randint(0, n_actions, n_states)\n",
    "        \n",
    "    return rand_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action frequencies over 10^4 samples: [ 0.1668264  0.1667818  0.166578   0.166587   0.166508   0.1667188]\n",
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "policies = [get_random_policy() for i in range(10**4)]\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should match n_actions-1'\n",
    "action_probas = np.unique(policies, return_counts=True)[-1] /10**4. /n_states\n",
    "print(\"Action frequencies over 10^4 samples:\",action_probas)\n",
    "assert np.allclose(action_probas, [1. / n_actions] * n_actions, atol=0.05), \"The policies aren't uniformly random (maybe it's just an extremely bad luck)\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate!\n",
    "* Implement a simple function that runs one game and returns the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_reward(env, policy, t_max=10000):\n",
    "    \"\"\"\n",
    "    Interact with an environment, return sum of all rewards.\n",
    "    If game doesn't end on t_max (e.g. agent walks into a wall), \n",
    "    force end the game and return whatever reward you got so far.\n",
    "    Tip: see signature of env.step(...) method above.\n",
    "    \"\"\"\n",
    "    #s: state; where our actor is\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    is_done = False\n",
    "\n",
    "    while t < t_max and not is_done:\n",
    "        # p = policy: with probabilities equal to the ones returned by get_random_policy()\n",
    "        s, reward, is_done, _ = env.step(policy[s])\n",
    "        # accumulating rewards\n",
    "        total_reward += reward\n",
    "        t+=1\n",
    "    #s = env.reset()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 10^3 sessions...\n",
      "Looks good!\n"
     ]
    }
   ],
   "source": [
    "print(\"generating 10^3 sessions...\")\n",
    "rewards = [sample_reward(env,get_random_policy()) for _ in range(10**3)]\n",
    "assert all([type(r) in (int, float) for r in rewards]), 'sample_reward must return a single number'\n",
    "#assert all([0 <= r <= 1 for r in rewards]), 'total rewards should be between 0 and 1 for frozenlake (if solving taxi, delete this line)'\n",
    "print(\"Looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(policy, n_times=20):\n",
    "    \"\"\"Run several evaluations and average the score the policy gets.\"\"\"\n",
    "    # rewards: array with n_times (100) elements consisting of the total_rewards returned by sample_reward()\n",
    "    rewards = [sample_reward(env, policy) for n in range(n_times)]\n",
    "    return float(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignoring the random search, jumping right into\n",
    "# Part II - Genetic Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    pass\n",
    "\n",
    "print_policy(get_random_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossover(policy1, policy2, p=0.5):\n",
    "    \"\"\"\n",
    "    for each state, with probability p take action from policy1, else policy2\n",
    "    \"\"\"\n",
    "    # policyx: [0,1,3,2,1,0,3,2,1,0,3,2,0,2,2,1]\n",
    "    new_pol = []\n",
    "    for i in range(len(policy1)):\n",
    "        #choosing between the ith element between pol1 and pol2 with probability p\n",
    "        new_pol.append(np.random.choice((policy1[i], policy2[i]), p=[p, 1-p]))\n",
    "        \n",
    "    return new_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mutation(policy, p=0.1):\n",
    "    \"\"\"\n",
    "    for each state, with probability p replace action with random action\n",
    "    Tip: mutation can be written as crossover with random policy\n",
    "    \"\"\"\n",
    "    #n_actions = env.action_space.n\n",
    "    for a in policy:\n",
    "        # with 10% probability, we mutate element a from policy\n",
    "        if np.random.choice((0,1), p=[1-p, p]):\n",
    "            policy[a] = np.random.randint(0, n_actions)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems fine!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "policies = [crossover(get_random_policy(), get_random_policy()) \n",
    "            for i in range(10**4)]\n",
    "\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should be n_actions-1'\n",
    "\n",
    "assert any([np.mean(crossover(np.zeros(n_states), np.ones(n_states))) not in (0, 1)\n",
    "               for _ in range(100)]), \"Make sure your crossover changes each action independently\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000 #default: 100 - how many cycles to make\n",
    "pool_size = 100 #how many policies to maintain\n",
    "n_crossovers = 50 #how many crossovers to make on each step\n",
    "n_mutations = 50 #how many mutations to make on each tick\n",
    "n_low_scorers = 0 #how many random low-scorer policies, not into the best scorers, can pass to the next pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing...\n"
     ]
    }
   ],
   "source": [
    "print(\"initializing...\")\n",
    "#pool = <spawn a list of pool_size random policies>\n",
    "pool = [get_random_policy() for i in range(pool_size)]\n",
    "#pool_scores = <evaluate every policy in the pool, return list of scores>\n",
    "pool_scores = [evaluate(p) for p in pool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert type(pool) == type(pool_scores) == list\n",
    "assert len(pool) == len(pool_scores) == pool_size\n",
    "assert all([type(score) in (float, int) for score in pool_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diverse Pool\n",
    "\n",
    "### As with 4.-moar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "Best score: -648.65\n",
      "\n",
      "Epoch 1:\n",
      "We found 11 duplicated policies at this epoch!\n",
      "Best score: -559.1\n",
      "\n",
      "Epoch 2:\n",
      "We found 19 duplicated policies at this epoch!\n",
      "Best score: -558.65\n",
      "\n",
      "Epoch 3:\n",
      "We found 17 duplicated policies at this epoch!\n"
     ]
    }
   ],
   "source": [
    "#main loop\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Epoch %s:\"%epoch)\n",
    "    \n",
    "    # 1. Removing duplicates from pool\n",
    "    # converting list of np arrays to list of tuples because I couldn't find a better way to remove duplicates\n",
    "    uniques_pool = [tuple(policy) for policy in pool]\n",
    "    # set() returns only unique elements. We need to convert them to np arrays again\n",
    "    uniques_pool = [np.asarray(policy) for policy in set(uniques_pool)]\n",
    "    if len(uniques_pool) != len(pool):\n",
    "        # We found some duplicates\n",
    "        print(\"We found\", len(pool) - len(uniques_pool), \"duplicated policies at this epoch!\")\n",
    "        pool = uniques_pool\n",
    "        # we should fill pool with new random policies to keep it size, but with the code below\n",
    "        # it will be maintain it's size.\n",
    "    # We could check for duplicates on crossovered or mutated,\n",
    "    #but it's more similar code and I want to finish this one\n",
    "    \n",
    "    # evaluation policies before crossovering them:\n",
    "    pool_scores = [evaluate(p) for p in pool]\n",
    "    # we'll select the best n_crossovers (50) policies to mix between them\n",
    "    # we could use another number of best policies instead of n_crossovers (50),\n",
    "    # but it's late and I don't know what I'm doing\n",
    "    selected_indices = np.argsort(pool_scores)[-n_crossovers:]\n",
    "    \n",
    "    #crossovered = <crossover random guys from pool, n_crossovers total>\n",
    "    # using selected_indices as a fraction of pool with 50 best scores\n",
    "    crossovered = [crossover(pool[np.random.choice(selected_indices)], \n",
    "                             pool[np.random.choice(selected_indices)]) \n",
    "                   for c in range(n_crossovers)]\n",
    "    # from now on it's all the same: mutations, adding all to a pool, evaluating (again) and selecting\n",
    "    # best scores. Repeat for n_epochs.\n",
    "    #mutated = <add several new policies at random, n_mutations total>\n",
    "    mutated = [mutation(pool[np.random.choice(len(pool))]) \n",
    "               for m in range(n_mutations)]\n",
    "    \n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "    \n",
    "    #add new policies to the pool\n",
    "    #pool = <add up old population with crossovers/mutations>\n",
    "    #plus sing (+) concatenates lists in python\n",
    "    pool = pool + crossovered + mutated\n",
    "    #pool_scores = <evaluate all policies again>\n",
    "    pool_scores = [evaluate(p) for p in pool]\n",
    "    \n",
    "    # 2. Adding a couple of random low scorers to the final pool\n",
    "    # select pool_size-n_low_scorers best policies. we'll add n_low_scorers later \n",
    "    selected_indices = np.argsort(pool_scores)[-pool_size+n_low_scorers:]\n",
    "    # Now we need to add n_low_scorers to our indices\n",
    "    # np.argsort(pool_scores)[:-pool_size+n_low_scorers] will contain all indices NOT used abobe\n",
    "    # so we need to choose n_low_scorers random indices from there\n",
    "    low_scorers_indices = np.random.choice(np.argsort(pool_scores)[:-pool_size+n_low_scorers], n_low_scorers)\n",
    "    # now we need to concatenate all indices into one numpy array\n",
    "    selected_indices = np.concatenate((selected_indices, low_scorers_indices))\n",
    "    #filling pool only with best values\n",
    "    pool = [pool[i] for i in selected_indices]\n",
    "    pool_scores = [pool_scores[i] for i in selected_indices]\n",
    "\n",
    "    #print the best policy so far (last in ascending score order)\n",
    "    print(\"Best score:\", pool_scores[-1])\n",
    "    print_policy(pool[-1])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from gym import wrappers\n",
    "#env = gym.make('CartPole-v0')\n",
    "#env = wrappers.Monitor(env, '/tmp/cartpole-experiment-1')\n",
    "#for i_episode in range(20):\n",
    "#    observation = env.reset()\n",
    "#    for t in range(100):\n",
    "#        env.render()\n",
    "#        print(observation)\n",
    "#        action = env.action_space.sample()\n",
    "#        observation, reward, done, info = env.step(action)\n",
    "#        if done:\n",
    "#            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
